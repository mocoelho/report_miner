{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### EXTRACTING REPORTS FROM DATA LAKE\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "SELECT patient_id, accession_number, exame_codigo, exame_desc, CAST(DATE(exam_datetime) AS DATE) AS exam_date, laudo_txt\n",
    "FROM `interoper-dataplatform-prd.raw_rdi.rdi_tb_carestream_laudos` \n",
    "WHERE DATE(exam_datetime) BETWEEN \"2021-01-01\" AND \"2021-06-18\"\n",
    "      \n",
    "      AND (exame_desc LIKE '%RM%' OR exame_desc LIKE '%RESSONANCIA%')  \n",
    "      \n",
    "      AND exame_desc LIKE '%CRANIO%' \n",
    "      AND (exame_desc NOT LIKE '%ANGIO%')\n",
    "      \n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "include_stopwords = ['com','sem']\n",
    "stopwords = [word for word in stopwords if not word in include_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show stopwords\n",
    "sorted(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report_file = 'RM_CRANIO_JAN_JUN.csv'\n",
    "report_file = 'RM_ABDOME_JAN_JUN.csv'\n",
    "df = pd.read_csv(report_file, delimiter=',')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [ 'exame_codigo', 'exame_desc']\n",
    "df = df.drop(columns=drop_columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpa_texto(texto: str):\n",
    "    # tira a acentuacao do texto. Coração -> Coracao\n",
    "    texto = unidecode.unidecode(texto)\n",
    "    \n",
    "    # formata o texto para evitar espacos, paragrafos, etc a mais\n",
    "    texto = replace_string(texto, '*', '')\n",
    "    texto = replace_string(texto, '..', '.')\n",
    "    texto = replace_string(texto, '  ', ' ')\n",
    "    texto = replace_string(texto, ' .', '.')\n",
    "    texto = replace_string(texto, ' ,', ',')\n",
    "    texto = replace_string(texto, '( ', '(')\n",
    "    texto = replace_string(texto, ' )', ')')\n",
    "    texto = replace_string(texto, ' :', ':')\n",
    "    texto = replace_string(texto, '\\\\n', '\\n')\n",
    "    texto = replace_string(texto, '\\t', '')\n",
    "    texto = replace_string(texto, '\\n ', '\\n')\n",
    "    texto = replace_string(texto, ' \\n', '\\n')\n",
    "    texto = replace_string(texto, '\\n.', '\\n')\n",
    "    texto = replace_string(texto, '\\n\\n\\n', '\\n\\n')\n",
    "    texto = texto.strip()\n",
    "    texto = texto.strip('\\n')\n",
    "    \n",
    "    # remove o nome do médico radiologista\n",
    "    texto = remove_doctor_name(texto)\n",
    "    \n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_string(texto: str, procurado, substituido: str):\n",
    "    # procura e troca as strings\n",
    "    \n",
    "    # faz a troca caso o parametro passado for uma string\n",
    "    if type(procurado) is str:\n",
    "        while procurado in texto:\n",
    "            texto = texto.replace(procurado, substituido)\n",
    "        return texto\n",
    "    \n",
    "    # faz a troca caso o parametro passado for uma lista\n",
    "    elif type(procurado) is list:\n",
    "        for palavra in procurado:\n",
    "            while palavra in texto:\n",
    "                texto = texto.replace(palavra, substituido)\n",
    "        return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_doctor_name(texto: str):\n",
    "    # palavras a serem buscadas no texto\n",
    "    doctor_words = ['\\n|Dr.', '\\n|Dra.', \"\\nDr.\", \"\\nDra.\"]\n",
    "    \n",
    "    # sai procurando pelas palavras de Dr(a).\n",
    "    for word in doctor_words:\n",
    "        if word in texto:\n",
    "            # se encontrou, localizada a posicao\n",
    "            pos = texto.find(word)\n",
    "            # faz um slice no texto\n",
    "            texto = texto[:pos]\n",
    "    \n",
    "    return texto        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lista_frases(texto: str, lista_frases_procuradas: list):\n",
    "    # gera uma lista de frases quebradas por paragrafo\n",
    "    \n",
    "    lista_return = []\n",
    "    \n",
    "    # splita o texto por paragrafo\n",
    "    frases_texto = texto.split('\\n')\n",
    "    \n",
    "    for frase in frases_texto:\n",
    "        for procurada in lista_frases_procuradas:\n",
    "            # se achou o texto procurado na frase\n",
    "            if procurada.upper() in frase.upper():\n",
    "                # adiciona o paragrafo na lista de paragrafos encontrados\n",
    "                lista_return.append(frase)\n",
    "                break\n",
    "    \n",
    "    if lista_return == []:\n",
    "        return ''\n",
    "    else:\n",
    "        return ' | '.join(lista_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_frases(texto: str, lista_frases_procuradas: list):\n",
    "    \n",
    "    lista_return = []\n",
    "    # Tira o texto da Técnica\n",
    "    texto = texto.replace('Tecnica:', '')\n",
    "    \n",
    "    # quebra as frases em paragrafos\n",
    "    texto = texto.replace('. ', '\\n')\n",
    "    \n",
    "    # splita o texto em paragrafos\n",
    "    texto = texto.split('\\n')\n",
    "    \n",
    "    for frase in texto:\n",
    "        # itera cada frase do texto\n",
    "        flag_remover = False\n",
    "        for procurada in lista_frases_procuradas:\n",
    "            if procurada.upper() in frase.upper():\n",
    "                # se achou, marca como uma frase a remover\n",
    "                flag_remover = True\n",
    "                break\n",
    "        \n",
    "        # inclui na lista de frases, apenas aquelas que nao foram marcadas como remover\n",
    "        if not flag_remover:\n",
    "            lista_return.append(frase)\n",
    "    \n",
    "    # remonta o laudo apenas com as frases que nao foram removidas\n",
    "    lista_return = '.\\n'.join(lista_return)\n",
    "    lista_return = limpa_texto(lista_return)\n",
    "    \n",
    "    return lista_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contem_frase(texto: str, lista_frases_procuradas: list):\n",
    "    # checa se uma frase está contida numa lista de frases\n",
    "    for procurada in lista_frases_procuradas:\n",
    "        if procurada.upper() in texto.upper():\n",
    "            return True\n",
    "                \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texto: str):\n",
    "    # garante que todo o conteúdo passado está em formato de lista\n",
    "    if type(texto) is str:\n",
    "        lista_texto = [texto]\n",
    "    elif type(texto) is list:\n",
    "        lista_texto = texto\n",
    "    else:\n",
    "        print(\"A função remove_stopwords aceita como parâmetro apenas strings ou listas.\")\n",
    "        exit()\n",
    "    \n",
    "    lista_removida = []        \n",
    "    for text in lista_texto:\n",
    "        # tira a acentuacao\n",
    "        text = unidecode.unidecode(text)\n",
    "        # tokeniza o texto tirando as stop words\n",
    "        text_tokens = nltk.tokenize.word_tokenize(text, language='portuguese') \n",
    "        tokens_without_sw = [word for word in text_tokens if not word in stopwords]\n",
    "        tokens_without_sw = ' '.join(tokens_without_sw)\n",
    "        tokens_without_sw = limpa_texto(tokens_without_sw)\n",
    "        # insere a frase limpa na lista frases sem stopwords\n",
    "        lista_removida.append(tokens_without_sw)\n",
    "    \n",
    "    # reconverte a saída para o mesmo formato da entrada\n",
    "    if type(texto) is str:\n",
    "        return lista_removida[0]\n",
    "    elif type(texto) is list:\n",
    "        return lista_removida\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define qual o nome da coluna do dataframe que contém o laudo\n",
    "campo_laudo = 'laudo_txt'\n",
    "\n",
    "# aplica a funcao limpa_texto para cada laudo de cada linha do dataframe\n",
    "df[campo_laudo] = df.apply(lambda row: limpa_texto(row[campo_laudo]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['laudo_txt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cria uma coluna com os laudos tokenizados. \n",
    "# essa funcao pode demorar alguns segundos para processar....\n",
    "\n",
    "# select the columns name containig the report text\n",
    "campo_laudo = 'laudo_txt'\n",
    "df['laudo_tokenizado'] = df.apply(lambda row: remove_stopwords(row[campo_laudo]), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['laudo_tokenizado'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_frases = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define as listas de inclusao e exclusao para detecao de contraste\n",
    "frases_inclusao = [\n",
    "                   'gadol', 'paramag', 'contrast'\n",
    "                  ]\n",
    "\n",
    "frases_exclusao = [ 'impregnacao', \n",
    "                    'nao administracao',\n",
    "                    'nao autoriz',\n",
    "                    'Nao foi administrado',\n",
    "                    'Nao foi autoriz',\n",
    "                    'nao foi injetado', \n",
    "                    'Nao foi obtida autorizacao',                  \n",
    "                    'nao foi realizad', \n",
    "                    'nao foi utiliz', \n",
    "                    'Nao houve autorizacao',\n",
    "                    'realce', \n",
    "                   'capta',\n",
    "                   'reforc',\n",
    "                    'realizado sem', \n",
    "                    'respeitando a opcao formal',\n",
    "                    'sem administracao',\n",
    "                    'sem contraste',\n",
    "                    'sem o uso',\n",
    "                   'sem injecao' \n",
    "                  ]\n",
    "\n",
    "dict_frases['contraste'] = {\"inclusao\": frases_inclusao, \"exclusao\": frases_exclusao}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frases de inclusao e exclusao para glioma\n",
    "frases_inclusao = ['glioma', 'gbm', 'neoplasia', 'glioblastoma', 'glial']\n",
    "\n",
    "frases_exclusao = [ 'mening', 'menigo',\n",
    "                   'p.o', 'cirur', 'operat',\n",
    "                   'Schwannoma',\n",
    "                   'mama', 'pulm', 'prost', 'renal', 'laringe', 'colo', 'ovari', 'intest', 'tiroid', 'tireo', 'oral', 'faringe','paragang', 'uter', 'gastr', 'estomago', 'mediast',\n",
    "                   'secund', 'metast',\n",
    "                   'Nao ha evidencia', 'Nao ha sugestao'\n",
    "                  ]\n",
    "\n",
    "dict_frases['glioma'] = {\"inclusao\": frases_inclusao, \"exclusao\": frases_exclusao}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frases de inclusao e exclusao para hcc\n",
    "frases_inclusao = ['hcc', 'chc', 'hepatocelular', 'hepato-celular', 'hepato celular', 'carcinoma']\n",
    "\n",
    "frases_exclusao = [ 'pancreas', 'renais', 'celulas claras', 'histori', 'hiperplasia', 'nefro', 'carcinomatos', 'adenocarcinoma',\n",
    "                   'secund', 'metast', 'colangio', 'hemangioma', 'hnf', 'uro', 'ureter', 'hepatocelular benigna', 'hepatocelulares benignas', 'tireoide', 'ovario',\n",
    "                   'reto', 'endometrio', 'vagina', 'uter',\n",
    "                   'Nao ha', 'Nao caracterizam', 'Nao caracterizadas', 'Nao concentracao', 'Nao evidenciam',\n",
    "                   'Nao ha esteatose, sobrecarga ferrica lesoes suspeitas hepatocarcinoma',\n",
    "                   'Nao ha nodulos suspeitos hepatocarcinomas', 'nao observam', 'Nao caracterizado', 'Nao evidenciad', 'Nao surgi',\n",
    "                   'Nao ha esteatose, sobrecarga ferrica lesoes suspeitas hepatocarcinoma', \n",
    "                   'Ausencia les', 'sem lesoes suspeitas','Ausencia outras lesoes suspeitas CHC',\n",
    "                   'rastreio'\n",
    "                  ]\n",
    "dict_frases['hcc'] = {\"inclusao\": frases_inclusao, \"exclusao\": frases_exclusao}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defina na linha abaixo qual dicionario deseja procurar\n",
    "dict_interesse = 'hcc'\n",
    "\n",
    "frases_inclusao = remove_stopwords(dict_frases[dict_interesse][\"inclusao\"])\n",
    "frases_exclusao = remove_stopwords(dict_frases[dict_interesse][\"exclusao\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_frases[dict_interesse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cria uma nova coluna contendo o laudo sem as frases de exclusao\n",
    "df['laudo_pos_exclusao'] = df.apply(lambda row: remove_frases(row['laudo_tokenizado'], frases_exclusao), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['laudo_pos_exclusao'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cria uma nova coluna contendo apenas as frases de inclusao que foram encontradas no laudo\n",
    "df['frases_inclusao'] = df.apply(lambda row: lista_frases(row['laudo_pos_exclusao'], frases_inclusao), axis = 1) \n",
    "df['frases_inclusao'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in list(df['frases_inclusao'].drop_duplicates().sort_values(ignore_index = True)):\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['laudo_tokenizado', 'laudo_pos_exclusao']\n",
    "df_export = df.drop(columns=drop_columns)\n",
    "\n",
    "novo_nome_arq, _ = os.path.splitext(report_file) \n",
    "novo_nome_arq += \"_NEW2.csv\"\n",
    "df_export.to_csv(novo_nome_arq, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
